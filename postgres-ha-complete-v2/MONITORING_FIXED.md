# PostgreSQL HA Cluster v2 - Monitoring Fixed âœ…

## ðŸŽ¯ Issue Resolved
**Problem**: Grafana dashboard had no data despite cluster running and syncing correctly.

**Root Cause**: 
1. Single postgres-exporter misconfigured with invalid multi-server DSN
2. Prometheus scrape jobs pointing to raw PostgreSQL ports (5432-5442) instead of exporter
3. Prometheus container not connected to ha-network where exporters could reach cluster

**Solution**: 
- Created 6 individual postgres-exporter instances (one per PostgreSQL node)
- Updated prometheus.yml to scrape each exporter on correct port (9187)
- Connected Prometheus container to ha-network for inter-container communication

---

## ðŸ“Š Current System Status

### PostgreSQL Cluster (All Healthy âœ…)
| Node | Type | Port | Status | Replication Lag |
|------|------|------|--------|-----------------|
| pg-primary | Primary | 5432 | Up 46+ min | 0s |
| pg-backup1 | Standby | 5433 | Up 47+ min | 0s |
| pg-backup2 | Standby | 5434 | Up 47+ min | 0s |
| pg-ro1 | Read-Only | 5440 | Up 44+ min | 0s |
| pg-ro2 | Read-Only | 5441 | Up 40+ min | 0s |
| pg-ro3 | Read-Only | 5442 | Up 44+ min | 0s |

**Replication Status**: All nodes synced in real-time (0s lag across all replicas)

**Database Size** (appdb):
- Primary: 7.62 MB
- Backups: ~7.42-7.63 MB  
- Read-Only: ~7.63 MB
- **All synchronized** âœ…

### Monitoring Stack (All Running âœ…)

| Service | Container | Port | Status | Role |
|---------|-----------|------|--------|------|
| Prometheus | prometheus-cluster | 9090 | Up 5+ min | Metrics aggregation |
| Grafana | grafana-cluster | 3000 | Up 5+ min | Dashboard visualization |
| Exporter-Primary | exporter-primary | 9187 | Up 5+ min | Scrapes pg-primary |
| Exporter-Backup1 | exporter-backup1 | 9188 | Up 5+ min | Scrapes pg-backup1 |
| Exporter-Backup2 | exporter-backup2 | 9189 | Up 5+ min | Scrapes pg-backup2 |
| Exporter-RO1 | exporter-ro1 | 9190 | Up 5+ min | Scrapes pg-ro1 |
| Exporter-RO2 | exporter-ro2 | 9191 | Up 5+ min | Scrapes pg-ro2 |
| Exporter-RO3 | exporter-ro3 | 9192 | Up 5+ min | Scrapes pg-ro3 |

---

## ðŸ“ˆ Available Metrics in Prometheus

**Replication Metrics** (now collected from all 6 nodes):
- `pg_replication_lag_seconds` - Replication lag per node (all: 0s)
- `pg_replication_is_replica` - Boolean per node
- `pg_replication_slot_slot_current_wal_lsn` - WAL position
- `pg_replication_slots_pg_wal_lsn_diff` - WAL difference
- `pg_replication_slots_active` - Active slots count

**Database Metrics**:
- `pg_database_size_bytes` - Database size per database per node
- `pg_database_connection_limit` - Connection limits

**System Metrics**:
- `pg_locks_count` - Active locks
- `pg_settings_*` - PostgreSQL configuration settings
- `pg_scrape_collector_*` - Exporter scrape performance
- `pg_exporter_*` - Exporter-specific metrics

**Total Metrics**: 24 time series for `pg_database_size_bytes` (4 databases Ã— 6 nodes)

---

## ðŸ”§ Architecture Changes Made

### Before (Broken)
```
[Prometheus] â”€â”€Ã— (can't connect)
                 â””â”€ postgres-exporter (1 instance)
                    â””â”€ [Invalid DSN: postgresql://...@host1,host2,host3]
                    â””â”€ âœ— Fails to connect to PostgreSQL nodes
```

### After (Fixed)
```
[Prometheus] â”€â”€(monitoring network)
   â†“
[HA Network]
   â”œâ”€ exporter-primary:9187 â†’ pg-primary:5432 âœ…
   â”œâ”€ exporter-backup1:9188 â†’ pg-backup1:5432 âœ…
   â”œâ”€ exporter-backup2:9189 â†’ pg-backup2:5432 âœ…
   â”œâ”€ exporter-ro1:9190 â†’ pg-ro1:5432 âœ…
   â”œâ”€ exporter-ro2:9191 â†’ pg-ro2:5432 âœ…
   â””â”€ exporter-ro3:9192 â†’ pg-ro3:5432 âœ…
```

---

## ðŸš€ How to Access

**Grafana Dashboard**:
- URL: http://localhost:3000
- Username: admin
- Password: admin
- Datasource: Prometheus (http://prometheus-cluster:9090)
- Dashboard: PostgreSQL HA Cluster (auto-provisioned)

**Prometheus**:
- URL: http://localhost:9090
- Targets: All 8 services (Prometheus + 6 exporters + Grafana auto-discovery)
- Scrape Interval: 15 seconds
- Evaluation Interval: 15 seconds

---

## âœ… Verification Checklist

- [x] All 6 PostgreSQL nodes running healthy
- [x] All 6 postgres-exporter instances running and connected
- [x] Prometheus successfully scraping from all exporters
- [x] Replication lag showing 0s across all nodes
- [x] Database size consistent across all 6 nodes
- [x] Metrics visible in Prometheus queries
- [x] Grafana datasource connected to Prometheus
- [x] PostgreSQL HA dashboard provisioned
- [x] Container networks properly configured
- [x] Cross-network communication working (monitoring â†’ ha-network)

---

## ðŸ“ Files Modified

1. **monitoring/docker-compose.yml**
   - Replaced single `postgres-exporter` with 6 individual exporters
   - Each exporter on unique port (9187-9192)
   - Each with correct connection string to respective PostgreSQL node
   - Connected to both `monitoring` and `ha-network` networks

2. **monitoring/prometheus.yml**
   - Removed raw PostgreSQL port jobs (5432-5442)
   - Added 6 jobs for postgres-exporter instances
   - Labels for instance identification and role tagging

3. **Network Configuration**
   - Connected prometheus-cluster to ha-network for cluster communication
   - All exporters already connected to ha-network
   - Enabled cross-network communication between monitoring and cluster networks

---

## ðŸ”„ Data Flow

```
PostgreSQL Nodes (HA Network)
    â†“ (streaming replication)
All 6 Nodes Synced
    â†“
postgres-exporter instances (metric scrape)
    â†“ (on demand)
Prometheus (metric storage + TS DB)
    â†“ (dashboard queries)
Grafana Dashboard
    â†“
User Visualization
```

---

## ðŸŽ“ Key Learnings

1. **Multiple exporters needed** for monitoring multi-node clusters
2. **Network segmentation** requires explicit network connections for cross-project communication
3. **Container DNS** works within networks (exporter-primary:9187 resolves within ha-network)
4. **Label strategy** important for identifying metrics source in Prometheus

---

## ðŸš¦ Next Steps (Optional)

1. Create custom Grafana dashboard with:
   - Replication lag trending
   - Write throughput (rows/sec)
   - Connection counts per node
   - WAL position tracking

2. Set up alerting rules for:
   - Replication lag > 1s
   - Node down detection
   - Database size anomalies

3. Configure retention policies for:
   - WAL archiving
   - Prometheus metrics retention

4. Add more test workloads:
   - Read-heavy vs write-heavy scenarios
   - Failover simulation
   - Performance benchmarking

---

**Status**: âœ… MONITORING FULLY OPERATIONAL

Last Updated: 2026-01-18 17:50 UTC
